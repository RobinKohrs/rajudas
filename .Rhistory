gsub("[^A-Za-z0-9 ]", "", subtitle)
gsub("[^A-Za-z0-9 öäüÖÜÄ]", "", subtitle)
gsub("[^A-Za-z0-9 öäüÖÜÄ/]", "", subtitle)
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", subtitle)
subtitle = article %>% rvest::html_elements(".teaser-subtitle") %>% rvest::html_text() %>% gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .)
subtitle
article %>% rvest::html_element("img")
article %>% rvest::html_element("img") %>% rvest::html_attr("data-lazy-src")
img_src = article %>% rvest::html_element("img") %>% rvest::html_attr("data-lazy-src")
list(
title
)
article_attributes = list(
title = article %>%
rvest::html_element(".teaser-title") %>%
rvest::html_text(),
subtitle = article %>%
rvest::html_element(".teaser-subtitle") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
postings = article %>%
rvest::html_element(".teaser-postingcount") %>%
rvest::html_text() %>%
stringr::str_extract("[0-9]+") %>%
as.numeric(),
link = article %>%
rvest::html_element("a") %>%
rvest::html_attr("href") %>%
paste0("https://www.derstandard.at", .),
img_src = article %>%
rvest::html_element("img") %>%
rvest::html_attr("data-lazy-src")
)
article_attributes
as = purrr::map(articles, function(article){
get_article_attributes(article)
})
get_article_attributes = function(article){
article_attributes = list(
title = article %>%
rvest::html_element(".teaser-title") %>%
rvest::html_text(),
subtitle = article %>%
rvest::html_element(".teaser-subtitle") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
postings = article %>%
rvest::html_element(".teaser-postingcount") %>%
rvest::html_text() %>%
stringr::str_extract("[0-9]+") %>%
as.numeric(),
link = article %>%
rvest::html_element("a") %>%
rvest::html_attr("href") %>%
paste0("https://www.derstandard.at", .),
img_src = article %>%
rvest::html_element("img") %>%
rvest::html_attr("data-lazy-src")
)
return(article_attributes)
}
as = purrr::map(articles, function(article){
get_article_attributes(article)
})
bind_rows(as)
get_article_attributes = function(article){
article_attributes = list(
title = article %>%
rvest::html_element(".teaser-title") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
subtitle = article %>%
rvest::html_element(".teaser-subtitle") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
postings = article %>%
rvest::html_element(".teaser-postingcount") %>%
rvest::html_text() %>%
stringr::str_extract("[0-9]+") %>%
as.numeric(),
link = article %>%
rvest::html_element("a") %>%
rvest::html_attr("href") %>%
paste0("https://www.derstandard.at", .),
img_src = article %>%
rvest::html_element("img") %>%
rvest::html_attr("data-lazy-src")
)
return(article_attributes)
}
as = purrr::map(articles, function(article){
get_article_attributes(article)
})
bind_rows(as)
articles=articles[[]]
articles=articles[[1]]
article_attributes = list(
title = article %>%
rvest::html_element(".teaser-title") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
subtitle = article %>%
rvest::html_element(".teaser-subtitle") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
postings = article %>%
rvest::html_element(".teaser-postingcount") %>%
rvest::html_text() %>%
stringr::str_extract("[0-9]+") %>%
as.numeric() %>% ifelse(is.na(.), 0, .),
link = article %>%
rvest::html_element("a") %>%
rvest::html_attr("href") %>%
paste0("https://www.derstandard.at", .),
img_src = article %>%
rvest::html_element("img") %>%
rvest::html_attr("data-lazy-src")
)
article_attributes = list(
title = article %>%
rvest::html_element(".teaser-title") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
subtitle = article %>%
rvest::html_element(".teaser-subtitle") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
postings = article %>%
rvest::html_element(".teaser-postingcount") %>%
rvest::html_text() %>%
stringr::str_extract("[0-9]+") %>%
as.numeric() %>% ifelse(is.na(.), 0, .),
link = article %>%
rvest::html_element("a") %>%
rvest::html_attr("href") %>%
paste0("https://www.derstandard.at", .),
img_src = article %>%
rvest::html_element("img") %>%
rvest::html_attr("data-lazy-src")
)
article_attributes
as = purrr::map(articles, function(article){
get_article_attributes(article)
})
get_article_attributes = function(article){
article_attributes = list(
title = article %>%
rvest::html_element(".teaser-title") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
subtitle = article %>%
rvest::html_element(".teaser-subtitle") %>%
rvest::html_text() %>%
gsub("[^A-Za-z0-9 öäüÖÜÄ/-]", "", .),
postings = article %>%
rvest::html_element(".teaser-postingcount") %>%
rvest::html_text() %>%
stringr::str_extract("[0-9]+") %>%
as.numeric() %>% ifelse(is.na(.), 0, .),
link = article %>%
rvest::html_element("a") %>%
rvest::html_attr("href") %>%
paste0("https://www.derstandard.at", .),
img_src = article %>%
rvest::html_element("img") %>%
rvest::html_attr("data-lazy-src")
)
return(article_attributes)
}
as = purrr::map(articles, function(article){
get_article_attributes(article)
})
# read it in
rvest::read_html(downloaded_html_file, encoding = "UTF-8") %>%
rvest::html_elements("article") -> articles
as = purrr::map(articles, function(article){
get_article_attributes(article)
})
as
bind_rows(a)
bind_rows(as)
as = purrr::map(articles, function(article){
get_article_attributes(article)
}) %>% dpylr::bind_rows()
as = purrr::map(articles, function(article){
get_article_attributes(article)
}) %>% dplyr::bind_rows()
as
devtools::load_all()
library(googlesheets4)
?write_sheet()
write_sheet(as, "https://docs.google.com/spreadsheets/d/1bwQSg18ByDHllBA8KR578EZa8mP5A3iHUBvnmAZs7aE/edit?gid=0#gid=0", "test")
write_sheet(as, "https://docs.google.com/spreadsheets/d/1bwQSg18ByDHllBA8KR578EZa8mP5A3iHUBvnmAZs7aE/edit?gid=0#gid=0", "articles")
df = data.frame(
c1 = c("a", "b", "c"),
c2 = list("a", "b", "c")
)
df
data_frame(abc = letters[1:3], lst = list(1:3, 1:3, 1:3))
letters[1:3]
df = data.frame(
c1 = c("a", "b", "c"),
c2 = list(1:3, 1:3, 1:3)
)
df
df = tibble(
c1 = c("a", "b", "c"),
c2 = list(1:3, 1:3, 1:3)
)
df
df = tibble(
c1 = c("a", "b", "c"),
c2 = list(c("a", "b"), c("d", "e"), c("e", "f"))
)
df
df$c2
df = tibble(
c1 = c("a", "b", "c"),
c2 = list(c("a", "b"), c("d", "e"), c("e", "f")),
c3 = list(c("g", "h"), c("i", "j"), c("k", "l")),
)
df
df %>%
mutate(
labels = map2_chr(c2, c3, ~ paste(paste(.x, .y, sep = ","), collapse = "<br>"))
)
paste(c("a", "b", "c"), sep="||")
paste(c("a", "b", "c"), collapse="||")
?paste
paste(1:12)
paste(1:12, sep=",")
(nth <- paste0(1:12, c("st", "nd", "rd", rep("th", 9))))
paste0(1:12, c("st", "nd", "rd", rep("th", 9)))
paste0(1:12, c("st", "nd", "rd", rep("th", 9)), sep=",")
paste(1:12, c("st", "nd", "rd", rep("th", 9)), sep=",")
paste(1:12, c("st", "nd", "rd", rep("th", 9)), sep=",", collapse = "|")
paste0(c(1,2,3), collapse = "   ")
# -------------------------------------------------------------------------
# Set options (can be placed in a setup/init script)
# -------------------------------------------------------------------------
options(timeout = 999999)
get_latest_file <- function(host = c("arome", "nowcast"), base_url = "ftp://eaftp.zamg.ac.at/") {
host <- match.arg(host)
host_url <- paste0(base_url, ifelse(host == "arome", "nwp/", "nowcast/"))
current_files <- RCurl::getURL(
url = host_url,
verbose = TRUE,
ftp.use.epsv = TRUE,
dirlistonly = TRUE
)
files_clean <- strsplit(current_files, "\n")[[1]]
latest_file <- sort(files_clean)[length(files_clean)]
return(list(
name = latest_file,
remote_path = file.path(host_url, latest_file)
))
}
get_raster_data_at_point <- function(local_path,
subdataset = "T",
x, y,
level = NULL,
timezone = "Europe/Paris") {
data <- terra::rast(local_path, subds = subdataset)
# Select specific level if requested
if (!is.null(level)) {
level_pattern <- glue::glue("T_level={level}")
level_indices <- grep(level_pattern, names(data))
data <- data[[level_indices]]
}
# Get times and extract data
times <- terra::time(data)
times_converted <- lubridate::with_tz(times, timezone)
extracted <- terra::extract(data, data.frame(x = x, y = y)) |>
dplyr::select(-ID)
df <- data.frame(
value = purrr::map_dbl(unname(extracted), 1),
time = times_converted
)
return(df)
}
download_and_process_point_data <- function(host,
x, y,
level = NULL,
subdataset = "T",
download_dir = "data_raw",
timezone = "Europe/Paris") {
latest_file <- get_latest_file(host)
local_path <- file.path(download_dir, host, latest_file$name)
if (!file.exists(local_path)) {
dir.create(dirname(local_path), recursive = TRUE, showWarnings = FALSE)
download.file(latest_file$remote_path, destfile = local_path)
} else {
message("File already exists: ", local_path)
}
data <- get_raster_data_at_point(local_path,
subdataset = subdataset,
x = x, y = y,
level = level,
timezone = timezone)
return(data)
}
host = "arome"
x = 16.3970
y = 48.2075
level = "1000"
subdataset = "T"
download_dir = "data_raw"
timezone = "Europe/Paris"
latest_file <- get_latest_file(host)
latest_file
devtools::document()
host = c("nwp", "nowcast", "ensemble")
devtools::document()
devtools::document()
?geosphere_get_prediction
host="nwp"
base_url = "ftp://eaftp.zamg.ac.at/"
host <- match.arg(host)
host = c("nwp", "nowcast", "ensemble")
host <- match.arg(host)
pkgload::dev_help('geosphere_get_prediction')
host <- match.arg(host, c("a"))
get_data_type <- function(type = c("summary", "detailed", "full")) {
type <- match.arg(type)
print(glue::glue("You selected type: {type}"))
}
get_data_type("detailed")     # works
get_data_type("summ")         # works (partial match to "summary")
get_data_type()               # works (defaults to "summary")
get_data_type("invalid")      # ❌ error: must be one of "summary", "detailed", "full"
devtools::load_all()
get_latest_file("nowcast")
get_latest_file("ensemble")
get_latest_file = function(host = c("nwp", "nowcast", "ensemble"), base_url = "ftp://eaftp.zamg.ac.at/") {
host = match.arg(host)
host_url = paste0(base_url, host)
print(host_url)
current_files = RCurl::getURL(
url = host_url,
verbose = TRUE,
ftp.use.epsv = TRUE,
dirlistonly = TRUE
)
files_clean = strsplit(current_files, "\n")[[1]]
latest_file = sort(files_clean)[length(files_clean)]
return(list(
name = latest_file,
remote_path = file.path(host_url, latest_file)
))
}
get_latest_file("ensemble")
host_url="ftp://eaftp.zamg.ac.at/ensemble"
current_files = RCurl::getURL(
url = host_url,
verbose = TRUE,
ftp.use.epsv = TRUE,
dirlistonly = TRUE
)
current_files
host_url
host_url
current_files = RCurl::getURL(
url = host_url,
verbose = TRUE,
ftp.use.epsv = TRUE,
dirlistonly = FALSE
)
current_files = RCurl::getURL(
url = host_url,
verbose = TRUE,
ftp.use.epsv = TRUE,
dirlistonly = TRUE
)
current_files
host_url = paste0(base_url, host, "/")
host_url
host="ensemble"
host_url = paste0(base_url, host, "/")
host="ensemble"
host_url
current_files = RCurl::getURL(
url = host_url,
verbose = TRUE,
ftp.use.epsv = TRUE,
dirlistonly = TRUE
)
current_files
files_clean = strsplit(current_files, "\n")[[1]]
files_clean
latest_file = sort(files_clean)[length(files_clean)]
latest_file
latest_file
latest_file=list(
name = latest_file,
remote_path = file.path(host_url, latest_file)
)
# download the file to local
local_path = file.path(download_dir, host, latest_file$name)
local_path
# download the file to local
local_path = file.path(download_dir, host, latest_file$name)
if (!file.exists(local_path)) {
dir.create(dirname(local_path), recursive = TRUE, showWarnings = FALSE)
download.file(latest_file$remote_path, destfile = local_path)
} else {
message("File already exists: ", local_path)
}
local_path
library(devtools)
install_github("JoshOBrien/gdalUtilities")
usethis::create_github_token()
install.packages("gdalUtilities")
gdalUtilities::gdalinfo(local_path)
get_subdataset_names <- function(nc_path) {
info <- gdalUtilities::gdalinfo(nc_path, raw_output = TRUE)
# extract lines containing "SUBDATASET_*_NAME"
subds_lines <- info[str_detect(info, "SUBDATASET_\\d+_NAME")]
# extract the variable names (e.g., mxt2m_p90)
names <- str_extract(subds_lines, "(?<=:)[^:]+$")
return(names)
}
get_subdataset_names(local_path)
get_subdataset_names <- function(nc_path) {
info <- gdalUtilities::gdalinfo(nc_path)
# extract lines containing "SUBDATASET_*_NAME"
subds_lines <- info[str_detect(info, "SUBDATASET_\\d+_NAME")]
# extract the variable names (e.g., mxt2m_p90)
names <- str_extract(subds_lines, "(?<=:)[^:]+$")
return(names)
}
get_subdataset_names(local_path)
?gdalUtilities::gdalinfo(local_path)
info = gdalUtilities::gdalinfo(local_path)
info
subds_lines <- info[str_detect(info, "SUBDATASET_\\d+_NAME")]
subds_lines
gregexpr("SUBDATASET_\\d+_NAME=NETCDF:\\\\\"[^\"]+:(\\w+)", info)
# Your long string (abbreviated here; replace with your full version)
text <- "Driver: netCDF/Network Common Data Format\\nFiles: data_raw/ensemble/ensemble_2025040800.nc\\n...SUBDATASET_1_NAME=NETCDF:\\\"data_raw/ensemble/ensemble_2025040800.nc\\\":cape_p10\\n  SUBDATASET_1_DESC=...\\n..."
# Extract all lines with SUBDATASET_*_NAME
matches <- regmatches(text, gregexpr("SUBDATASET_\\d+_NAME=NETCDF:\\\\\"[^\"]+:(\\w+)", text))
# Flatten and extract the variable names using sub()
variable_names <- sub(".*:(\\w+)", "\\1", unlist(matches))
# View the result
print(variable_names)
lines <- strsplit(indo, "\n")[[1]]
lines <- strsplit(info, "\n")[[1]]
lines
start_idx <- which(lines == "Subdatasets:")
end_idx <- which(lines == "Corner Coordinates:") - 1
start_idx
end_idx
subdataset_lines <- lines[(start_idx+1):end_idx]
subdataset_lines
subdatasets <- data.frame(
index = integer(),
name = character(),
variable = character(),
description = character(),
dimensions = character(),
data_type = character(),
stringsAsFactors = FALSE
)
name_lines <- subdataset_lines[grep("_NAME=", subdataset_lines)]
for (i in seq_along(name_lines)) {
# Extract index number
idx <- as.integer(gsub(".*SUBDATASET_([0-9]+)_NAME=.*", "\\1", name_lines[i]))
# Extract full name
full_name <- gsub(".*SUBDATASET_[0-9]+_NAME=", "", name_lines[i])
# Extract variable name (the part after the last colon)
variable <- gsub(".*:", "", full_name)
# Add to data frame
subdatasets[i, "index"] <- idx
subdatasets[i, "name"] <- full_name
subdatasets[i, "variable"] <- variable
}
subdatasets
desc_lines <- subdataset_lines[grep("_DESC=", subdataset_lines)]
for (i in seq_along(desc_lines)) {
# Extract dimensions
dimensions <- gsub(".*\\[([^\\]]+)\\].*", "\\1", desc_lines[i])
# Extract description and data type
desc_and_type <- gsub(".*\\] ([^(]+) \\(([^)]+)\\)", "\\1|\\2", desc_lines[i])
parts <- strsplit(desc_and_type, "\\|")[[1]]
description <- parts[1]
data_type <- parts[2]
# Add to data frame
subdatasets[i, "description"] <- description
subdatasets[i, "dimensions"] <- dimensions
subdatasets[i, "data_type"] <- data_type
}
subdatasets
devtools::load_all()
devtools::load_all()
gdal_info_table(local_path)
gdal_info_table(local_path, interactive = T)
devtools::load_all()
gdal_info_table(local_path, interactive = T)
devtools::load_all()
devtools::document()
?geosphere_get_prediction
devtools::document()
?geosphere_get_prediction
devtools::document()
?geosphere_get_prediction
devtools::document()
?geosphere_get_prediction
devtools::document()
?geosphere_get_prediction
search()
?search()
install.packages("yahoofinancer")
